{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WWUwwNqwu9zS"
   },
   "source": [
    "## Import Data and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632
    },
    "colab_type": "code",
    "id": "aux7Xdpuu485",
    "outputId": "00434730-c6a9-43c0-eb5a-5c8904740f0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting catboost\n",
      "  Downloading catboost-1.2.7-cp38-cp38-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting graphviz (from catboost)\n",
      "  Downloading graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (from catboost) (3.3.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.8/site-packages (from catboost) (1.23.5)\n",
      "Requirement already satisfied: pandas>=0.24 in /opt/conda/lib/python3.8/site-packages (from catboost) (1.3.5)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from catboost) (1.10.1)\n",
      "Collecting plotly (from catboost)\n",
      "  Downloading plotly-5.24.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.24->catboost) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.24->catboost) (2024.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->catboost) (1.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->catboost) (8.4.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.8/site-packages (from matplotlib->catboost) (3.1.4)\n",
      "Collecting tenacity>=6.2.0 (from plotly->catboost)\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from plotly->catboost) (24.0)\n",
      "Downloading catboost-1.2.7-cp38-cp38-manylinux2014_x86_64.whl (98.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "Downloading plotly-5.24.1-py3-none-any.whl (19.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m185.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: tenacity, graphviz, plotly, catboost\n",
      "Successfully installed catboost-1.2.7 graphviz-0.20.3 plotly-5.24.1 tenacity-9.0.0\n",
      "Requirement already satisfied: lightgbm in /opt/conda/lib/python3.8/site-packages (4.5.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.8/site-packages (from lightgbm) (1.23.5)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from lightgbm) (1.10.1)\n",
      "Collecting missingpy\n",
      "  Downloading missingpy-0.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Downloading missingpy-0.2.0-py3-none-any.whl (49 kB)\n",
      "Installing collected packages: missingpy\n",
      "Successfully installed missingpy-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install catboost\n",
    "!pip install lightgbm\n",
    "!pip install missingpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VYqbGMy6vT5d"
   },
   "source": [
    "## Load Data and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cJeOV93kvQoN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report, f1_score\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "colab_type": "code",
    "id": "yknvEU0ZvhQj",
    "outputId": "ad53857c-1814-4b36-e277-5b0d5a7ec0a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13652, 72),\n",
       " Index(['days_from_entrance', 'age', 'document.sexo', 'UTI',\n",
       "        'delta_collect_timestamp_t-t1', 'delta_collect_timestamp_t1-t2',\n",
       "        'delta_collect_timestamp_t2-t3', 'delta_collect_timestamp_t3-t4',\n",
       "        'document.freq_cardiaca(t)', 'document.freq_cardiaca(t-1)',\n",
       "        'document.freq_cardiaca(t-2)', 'document.freq_cardiaca(t-3)',\n",
       "        'document.freq_cardiaca(t-4)', 'document.freq_respiratoria(t)',\n",
       "        'document.freq_respiratoria(t-1)', 'document.freq_respiratoria(t-2)',\n",
       "        'document.freq_respiratoria(t-3)', 'document.freq_respiratoria(t-4)',\n",
       "        'document.glicemia_capilar(t)', 'document.glicemia_capilar(t-1)',\n",
       "        'document.glicemia_capilar(t-2)', 'document.glicemia_capilar(t-3)',\n",
       "        'document.glicemia_capilar(t-4)', 'document.pa_diastolica(t)',\n",
       "        'document.pa_diastolica(t-1)', 'document.pa_diastolica(t-2)',\n",
       "        'document.pa_diastolica(t-3)', 'document.pa_diastolica(t-4)',\n",
       "        'document.pa_sistolica(t)', 'document.pa_sistolica(t-1)',\n",
       "        'document.pa_sistolica(t-2)', 'document.pa_sistolica(t-3)',\n",
       "        'document.pa_sistolica(t-4)', 'document.sat_o2(t)',\n",
       "        'document.sat_o2(t-1)', 'document.sat_o2(t-2)', 'document.sat_o2(t-3)',\n",
       "        'document.sat_o2(t-4)', 'document.temperatura(t)',\n",
       "        'document.temperatura(t-1)', 'document.temperatura(t-2)',\n",
       "        'document.temperatura(t-3)', 'document.temperatura(t-4)',\n",
       "        'delta_document.freq_cardiaca_t-t1',\n",
       "        'delta_document.freq_cardiaca_t1-t2',\n",
       "        'delta_document.freq_cardiaca_t2-t3',\n",
       "        'delta_document.freq_cardiaca_t3-t4',\n",
       "        'delta_document.freq_respiratoria_t-t1',\n",
       "        'delta_document.freq_respiratoria_t1-t2',\n",
       "        'delta_document.freq_respiratoria_t2-t3',\n",
       "        'delta_document.freq_respiratoria_t3-t4',\n",
       "        'delta_document.glicemia_capilar_t-t1',\n",
       "        'delta_document.glicemia_capilar_t1-t2',\n",
       "        'delta_document.glicemia_capilar_t2-t3',\n",
       "        'delta_document.glicemia_capilar_t3-t4',\n",
       "        'delta_document.pa_diastolica_t-t1',\n",
       "        'delta_document.pa_diastolica_t1-t2',\n",
       "        'delta_document.pa_diastolica_t2-t3',\n",
       "        'delta_document.pa_diastolica_t3-t4',\n",
       "        'delta_document.pa_sistolica_t-t1', 'delta_document.pa_sistolica_t1-t2',\n",
       "        'delta_document.pa_sistolica_t2-t3',\n",
       "        'delta_document.pa_sistolica_t3-t4', 'delta_document.sat_o2_t-t1',\n",
       "        'delta_document.sat_o2_t1-t2', 'delta_document.sat_o2_t2-t3',\n",
       "        'delta_document.sat_o2_t3-t4', 'delta_document.temperatura_t-t1',\n",
       "        'delta_document.temperatura_t1-t2', 'delta_document.temperatura_t2-t3',\n",
       "        'delta_document.temperatura_t3-t4', 'outcome'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"Dataset/heg_sample_data.csv\") #normalized dataset\n",
    "dataset.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "dataset.shape, dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DHQSXh9PwEpE"
   },
   "source": [
    "## Setup Expetiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WR0GF2tdvzMk"
   },
   "outputs": [],
   "source": [
    "X = dataset.drop([\"outcome\"], axis = 1)\n",
    "Y = dataset[\"outcome\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lv0hUf3kwSBC"
   },
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "\n",
    "classifiers = {\n",
    "    'XGBoost' : XGBClassifier(learning_rate=0.1, n_estimators=100,random_state=7, tree_method='gpu_hist'),\n",
    "    'LogReg': LogisticRegression(solver='liblinear', multi_class='ovr'),\n",
    "    'D.Tree': DecisionTreeClassifier(),\n",
    "    'RForest': RandomForestClassifier(n_estimators = 50),\n",
    "    'CatBoos': CatBoostClassifier(learning_rate=0.1,n_estimators=100,random_state=7,task_type='GPU',verbose = False),\n",
    "    'Naive': GaussianNB(),\n",
    "    'Light': lgb.LGBMClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8aTy1YKMwmom"
   },
   "source": [
    "## Run Basic Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "jVef0DcCwaYI",
    "outputId": "b1f93f80-91ca-42df-ab0c-58d7bd857540"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:28:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:28:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:28:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:28:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:28:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:28:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:28:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:28:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:28:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:28:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:28:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:28:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:28:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:28:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:28:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:28:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:28:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:28:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:28:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:28:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost\t 0.907 (0.5728) 111.93 s\n",
      "LogReg\t 0.8783 (0.5129) 10.94 s\n",
      "D.Tree\t 0.6953 (0.4507) 12.35 s\n",
      "RForest\t 0.8841 (0.5198) 40.49 s\n",
      "CatBoos\t 0.9058 (0.567) 28.6 s\n",
      "Naive\t 0.7854 (0.456) 0.24 s\n",
      "[LightGBM] [Info] Number of positive: 1504, number of negative: 10782\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001939 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10078\n",
      "[LightGBM] [Info] Number of data points in the train set: 12286, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122416 -> initscore=-1.969750\n",
      "[LightGBM] [Info] Start training from score -1.969750\n",
      "[LightGBM] [Info] Number of positive: 1512, number of negative: 10774\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001500 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10048\n",
      "[LightGBM] [Info] Number of data points in the train set: 12286, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.123067 -> initscore=-1.963703\n",
      "[LightGBM] [Info] Start training from score -1.963703\n",
      "[LightGBM] [Info] Number of positive: 1517, number of negative: 10770\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001395 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10062\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.123464 -> initscore=-1.960030\n",
      "[LightGBM] [Info] Start training from score -1.960030\n",
      "[LightGBM] [Info] Number of positive: 1496, number of negative: 10791\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001398 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10082\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.121755 -> initscore=-1.975918\n",
      "[LightGBM] [Info] Start training from score -1.975918\n",
      "[LightGBM] [Info] Number of positive: 1532, number of negative: 10755\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001421 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10074\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124685 -> initscore=-1.948797\n",
      "[LightGBM] [Info] Start training from score -1.948797\n",
      "[LightGBM] [Info] Number of positive: 1493, number of negative: 10794\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001410 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10058\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.121511 -> initscore=-1.978203\n",
      "[LightGBM] [Info] Start training from score -1.978203\n",
      "[LightGBM] [Info] Number of positive: 1524, number of negative: 10763\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001531 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10061\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124034 -> initscore=-1.954776\n",
      "[LightGBM] [Info] Start training from score -1.954776\n",
      "[LightGBM] [Info] Number of positive: 1524, number of negative: 10763\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002062 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10060\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124034 -> initscore=-1.954776\n",
      "[LightGBM] [Info] Start training from score -1.954776\n",
      "[LightGBM] [Info] Number of positive: 1504, number of negative: 10783\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001411 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10058\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122406 -> initscore=-1.969843\n",
      "[LightGBM] [Info] Start training from score -1.969843\n",
      "[LightGBM] [Info] Number of positive: 1532, number of negative: 10755\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001487 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10097\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124685 -> initscore=-1.948797\n",
      "[LightGBM] [Info] Start training from score -1.948797\n",
      "[LightGBM] [Info] Number of positive: 1504, number of negative: 10782\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001417 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10078\n",
      "[LightGBM] [Info] Number of data points in the train set: 12286, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122416 -> initscore=-1.969750\n",
      "[LightGBM] [Info] Start training from score -1.969750\n",
      "[LightGBM] [Info] Number of positive: 1512, number of negative: 10774\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001451 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10048\n",
      "[LightGBM] [Info] Number of data points in the train set: 12286, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.123067 -> initscore=-1.963703\n",
      "[LightGBM] [Info] Start training from score -1.963703\n",
      "[LightGBM] [Info] Number of positive: 1517, number of negative: 10770\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001832 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10062\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.123464 -> initscore=-1.960030\n",
      "[LightGBM] [Info] Start training from score -1.960030\n",
      "[LightGBM] [Info] Number of positive: 1496, number of negative: 10791\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001380 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10082\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.121755 -> initscore=-1.975918\n",
      "[LightGBM] [Info] Start training from score -1.975918\n",
      "[LightGBM] [Info] Number of positive: 1532, number of negative: 10755\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002044 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10074\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124685 -> initscore=-1.948797\n",
      "[LightGBM] [Info] Start training from score -1.948797\n",
      "[LightGBM] [Info] Number of positive: 1493, number of negative: 10794\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001394 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10058\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.121511 -> initscore=-1.978203\n",
      "[LightGBM] [Info] Start training from score -1.978203\n",
      "[LightGBM] [Info] Number of positive: 1524, number of negative: 10763\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001372 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10061\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124034 -> initscore=-1.954776\n",
      "[LightGBM] [Info] Start training from score -1.954776\n",
      "[LightGBM] [Info] Number of positive: 1524, number of negative: 10763\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002151 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10060\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124034 -> initscore=-1.954776\n",
      "[LightGBM] [Info] Start training from score -1.954776\n",
      "[LightGBM] [Info] Number of positive: 1504, number of negative: 10783\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002017 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10058\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122406 -> initscore=-1.969843\n",
      "[LightGBM] [Info] Start training from score -1.969843\n",
      "[LightGBM] [Info] Number of positive: 1532, number of negative: 10755\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002121 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10097\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124685 -> initscore=-1.948797\n",
      "[LightGBM] [Info] Start training from score -1.948797\n",
      "Light\t 0.9063 (0.5939) 4.03 s\n"
     ]
    }
   ],
   "source": [
    "for c in classifiers:\n",
    "  start = time.time()\n",
    "  model = classifiers[c]\n",
    "  scores = cross_val_score(model, X, Y, cv=kfold, scoring='roc_auc')\n",
    "  scores_f1 = cross_val_score(model, X, Y, cv=kfold, scoring='f1')\n",
    "  print (c + '\\t', round(scores.mean(),4), '(' + str(round(scores_f1.mean(),4)) + ')', round(time.time() - start,2), 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cG8GCzeNw8vd"
   },
   "source": [
    "## Cross Validation by Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 802
    },
    "colab_type": "code",
    "id": "IvQq8VhTw9fX",
    "outputId": "b4a3e4e9-f2b9-4236-be46-1242a42388d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Columns: 11 Exam(s): 1\n",
      "['document.sexo', 'document.pa_diastolica(t-4)', 'UTI', 'document.pa_sistolica(t-4)', 'document.freq_respiratoria(t-4)', 'document.glicemia_capilar(t-4)', 'age', 'document.temperatura(t-4)', 'document.sat_o2(t-4)', 'days_from_entrance', 'document.freq_cardiaca(t-4)']\n",
      "[10:31:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:31:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:31:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:31:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:31:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:31:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:31:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:31:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:31:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:31:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tXGBoost\t 0.8453 (+-0.0077) 2.4 s\n",
      "\tLogReg\t 0.8045 (+-0.015) 1.09 s\n",
      "\tD.Tree\t 0.6358 (+-0.0142) 0.6 s\n",
      "\tRForest\t 0.8109 (+-0.0131) 5.63 s\n",
      "\tCatBoos\t 0.8447 (+-0.0097) 11.74 s\n",
      "\tNaive\t 0.7696 (+-0.0138) 0.06 s\n",
      "[LightGBM] [Info] Number of positive: 1504, number of negative: 10782\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000250 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1171\n",
      "[LightGBM] [Info] Number of data points in the train set: 12286, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122416 -> initscore=-1.969750\n",
      "[LightGBM] [Info] Start training from score -1.969750\n",
      "[LightGBM] [Info] Number of positive: 1512, number of negative: 10774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000272 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1174\n",
      "[LightGBM] [Info] Number of data points in the train set: 12286, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.123067 -> initscore=-1.963703\n",
      "[LightGBM] [Info] Start training from score -1.963703\n",
      "[LightGBM] [Info] Number of positive: 1517, number of negative: 10770\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000244 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1167\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.123464 -> initscore=-1.960030\n",
      "[LightGBM] [Info] Start training from score -1.960030\n",
      "[LightGBM] [Info] Number of positive: 1496, number of negative: 10791\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000280 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1178\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.121755 -> initscore=-1.975918\n",
      "[LightGBM] [Info] Start training from score -1.975918\n",
      "[LightGBM] [Info] Number of positive: 1532, number of negative: 10755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000277 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1174\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124685 -> initscore=-1.948797\n",
      "[LightGBM] [Info] Start training from score -1.948797\n",
      "[LightGBM] [Info] Number of positive: 1493, number of negative: 10794\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000285 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1171\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.121511 -> initscore=-1.978203\n",
      "[LightGBM] [Info] Start training from score -1.978203\n",
      "[LightGBM] [Info] Number of positive: 1524, number of negative: 10763\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000262 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1173\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124034 -> initscore=-1.954776\n",
      "[LightGBM] [Info] Start training from score -1.954776\n",
      "[LightGBM] [Info] Number of positive: 1524, number of negative: 10763\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000270 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1177\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124034 -> initscore=-1.954776\n",
      "[LightGBM] [Info] Start training from score -1.954776\n",
      "[LightGBM] [Info] Number of positive: 1504, number of negative: 10783\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000269 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1174\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122406 -> initscore=-1.969843\n",
      "[LightGBM] [Info] Start training from score -1.969843\n",
      "[LightGBM] [Info] Number of positive: 1532, number of negative: 10755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000342 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1173\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124685 -> initscore=-1.948797\n",
      "[LightGBM] [Info] Start training from score -1.948797\n",
      "\tLight\t 0.8406 (+-0.0085) 0.93 s\n",
      "Number of Columns: 25 Exam(s): 2\n",
      "['document.freq_respiratoria(t-3)', 'delta_document.pa_sistolica_t3-t4', 'document.pa_sistolica(t-4)', 'age', 'document.sat_o2(t-4)', 'delta_document.temperatura_t3-t4', 'document.pa_diastolica(t-4)', 'document.pa_sistolica(t-3)', 'document.freq_respiratoria(t-4)', 'document.freq_cardiaca(t-3)', 'delta_document.sat_o2_t3-t4', 'document.sexo', 'UTI', 'document.sat_o2(t-3)', 'delta_document.freq_cardiaca_t3-t4', 'document.glicemia_capilar(t-4)', 'document.temperatura(t-4)', 'document.glicemia_capilar(t-3)', 'days_from_entrance', 'delta_document.pa_diastolica_t3-t4', 'delta_document.glicemia_capilar_t3-t4', 'delta_document.freq_respiratoria_t3-t4', 'document.pa_diastolica(t-3)', 'document.temperatura(t-3)', 'document.freq_cardiaca(t-4)']\n",
      "[10:31:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:31:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:31:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:31:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:31:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:31:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:31:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:31:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:31:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:31:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tXGBoost\t 0.8585 (+-0.0084) 2.54 s\n",
      "\tLogReg\t 0.8227 (+-0.0149) 1.93 s\n",
      "\tD.Tree\t 0.6524 (+-0.0105) 1.44 s\n",
      "\tRForest\t 0.8264 (+-0.0095) 9.43 s\n",
      "\tCatBoos\t 0.8579 (+-0.0075) 13.43 s\n",
      "\tNaive\t 0.7643 (+-0.0199) 0.1 s\n",
      "[LightGBM] [Info] Number of positive: 1504, number of negative: 10782\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000479 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3126\n",
      "[LightGBM] [Info] Number of data points in the train set: 12286, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122416 -> initscore=-1.969750\n",
      "[LightGBM] [Info] Start training from score -1.969750\n",
      "[LightGBM] [Info] Number of positive: 1512, number of negative: 10774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000386 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3130\n",
      "[LightGBM] [Info] Number of data points in the train set: 12286, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.123067 -> initscore=-1.963703\n",
      "[LightGBM] [Info] Start training from score -1.963703\n",
      "[LightGBM] [Info] Number of positive: 1517, number of negative: 10770\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000376 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3134\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.123464 -> initscore=-1.960030\n",
      "[LightGBM] [Info] Start training from score -1.960030\n",
      "[LightGBM] [Info] Number of positive: 1496, number of negative: 10791\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000382 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3139\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.121755 -> initscore=-1.975918\n",
      "[LightGBM] [Info] Start training from score -1.975918\n",
      "[LightGBM] [Info] Number of positive: 1532, number of negative: 10755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000394 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3133\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124685 -> initscore=-1.948797\n",
      "[LightGBM] [Info] Start training from score -1.948797\n",
      "[LightGBM] [Info] Number of positive: 1493, number of negative: 10794\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000373 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3132\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.121511 -> initscore=-1.978203\n",
      "[LightGBM] [Info] Start training from score -1.978203\n",
      "[LightGBM] [Info] Number of positive: 1524, number of negative: 10763\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000381 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3136\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124034 -> initscore=-1.954776\n",
      "[LightGBM] [Info] Start training from score -1.954776\n",
      "[LightGBM] [Info] Number of positive: 1524, number of negative: 10763\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000374 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3143\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124034 -> initscore=-1.954776\n",
      "[LightGBM] [Info] Start training from score -1.954776\n",
      "[LightGBM] [Info] Number of positive: 1504, number of negative: 10783\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000385 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3139\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122406 -> initscore=-1.969843\n",
      "[LightGBM] [Info] Start training from score -1.969843\n",
      "[LightGBM] [Info] Number of positive: 1532, number of negative: 10755\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000539 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3138\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124685 -> initscore=-1.948797\n",
      "[LightGBM] [Info] Start training from score -1.948797\n",
      "\tLight\t 0.8548 (+-0.0086) 1.23 s\n",
      "Number of Columns: 39 Exam(s): 3\n",
      "['document.freq_respiratoria(t-3)', 'delta_document.pa_sistolica_t3-t4', 'document.pa_sistolica(t-4)', 'delta_document.sat_o2_t2-t3', 'delta_document.pa_diastolica_t2-t3', 'age', 'document.sat_o2(t-4)', 'delta_document.freq_respiratoria_t2-t3', 'delta_document.temperatura_t3-t4', 'document.pa_diastolica(t-4)', 'delta_document.freq_cardiaca_t2-t3', 'document.freq_respiratoria(t-2)', 'document.pa_sistolica(t-3)', 'document.freq_respiratoria(t-4)', 'document.freq_cardiaca(t-3)', 'delta_document.sat_o2_t3-t4', 'document.sexo', 'UTI', 'document.pa_sistolica(t-2)', 'document.sat_o2(t-3)', 'delta_document.freq_cardiaca_t3-t4', 'document.glicemia_capilar(t-4)', 'delta_document.glicemia_capilar_t2-t3', 'document.temperatura(t-4)', 'document.glicemia_capilar(t-3)', 'document.glicemia_capilar(t-2)', 'days_from_entrance', 'delta_document.pa_diastolica_t3-t4', 'document.freq_cardiaca(t-2)', 'delta_document.pa_sistolica_t2-t3', 'document.pa_diastolica(t-2)', 'delta_document.glicemia_capilar_t3-t4', 'delta_document.freq_respiratoria_t3-t4', 'document.pa_diastolica(t-3)', 'document.temperatura(t-3)', 'document.sat_o2(t-2)', 'delta_document.temperatura_t2-t3', 'document.freq_cardiaca(t-4)', 'document.temperatura(t-2)']\n",
      "[10:32:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:32:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:32:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:32:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:32:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:32:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:32:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:32:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:32:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:32:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tXGBoost\t 0.8746 (+-0.0064) 3.01 s\n",
      "\tLogReg\t 0.8376 (+-0.0125) 2.82 s\n",
      "\tD.Tree\t 0.663 (+-0.0208) 2.53 s\n",
      "\tRForest\t 0.8454 (+-0.0146) 12.05 s\n",
      "\tCatBoos\t 0.8745 (+-0.007) 13.46 s\n",
      "\tNaive\t 0.7555 (+-0.0192) 0.09 s\n",
      "[LightGBM] [Info] Number of positive: 1504, number of negative: 10782\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001047 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5230\n",
      "[LightGBM] [Info] Number of data points in the train set: 12286, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122416 -> initscore=-1.969750\n",
      "[LightGBM] [Info] Start training from score -1.969750\n",
      "[LightGBM] [Info] Number of positive: 1512, number of negative: 10774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000486 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5221\n",
      "[LightGBM] [Info] Number of data points in the train set: 12286, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.123067 -> initscore=-1.963703\n",
      "[LightGBM] [Info] Start training from score -1.963703\n",
      "[LightGBM] [Info] Number of positive: 1517, number of negative: 10770\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000469 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5237\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.123464 -> initscore=-1.960030\n",
      "[LightGBM] [Info] Start training from score -1.960030\n",
      "[LightGBM] [Info] Number of positive: 1496, number of negative: 10791\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000484 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5251\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.121755 -> initscore=-1.975918\n",
      "[LightGBM] [Info] Start training from score -1.975918\n",
      "[LightGBM] [Info] Number of positive: 1532, number of negative: 10755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000781 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5227\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124685 -> initscore=-1.948797\n",
      "[LightGBM] [Info] Start training from score -1.948797\n",
      "[LightGBM] [Info] Number of positive: 1493, number of negative: 10794\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000476 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5220\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.121511 -> initscore=-1.978203\n",
      "[LightGBM] [Info] Start training from score -1.978203\n",
      "[LightGBM] [Info] Number of positive: 1524, number of negative: 10763\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000474 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5228\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124034 -> initscore=-1.954776\n",
      "[LightGBM] [Info] Start training from score -1.954776\n",
      "[LightGBM] [Info] Number of positive: 1524, number of negative: 10763\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000784 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5256\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124034 -> initscore=-1.954776\n",
      "[LightGBM] [Info] Start training from score -1.954776\n",
      "[LightGBM] [Info] Number of positive: 1504, number of negative: 10783\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000479 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5222\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122406 -> initscore=-1.969843\n",
      "[LightGBM] [Info] Start training from score -1.969843\n",
      "[LightGBM] [Info] Number of positive: 1532, number of negative: 10755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000484 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5236\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124685 -> initscore=-1.948797\n",
      "[LightGBM] [Info] Start training from score -1.948797\n",
      "\tLight\t 0.8765 (+-0.0091) 1.5 s\n",
      "Number of Columns: 53 Exam(s): 4\n",
      "['document.freq_respiratoria(t-3)', 'delta_document.pa_sistolica_t3-t4', 'document.pa_sistolica(t-4)', 'delta_document.sat_o2_t2-t3', 'delta_document.temperatura_t1-t2', 'delta_document.pa_diastolica_t2-t3', 'age', 'document.sat_o2(t-4)', 'document.pa_sistolica(t-1)', 'delta_document.sat_o2_t1-t2', 'delta_document.freq_respiratoria_t2-t3', 'delta_document.temperatura_t3-t4', 'document.pa_diastolica(t-4)', 'delta_document.freq_cardiaca_t2-t3', 'document.freq_respiratoria(t-2)', 'document.temperatura(t-1)', 'document.pa_sistolica(t-3)', 'document.freq_respiratoria(t-4)', 'document.freq_cardiaca(t-3)', 'delta_document.freq_cardiaca_t1-t2', 'delta_document.sat_o2_t3-t4', 'document.freq_respiratoria(t-1)', 'document.sexo', 'UTI', 'document.pa_sistolica(t-2)', 'document.sat_o2(t-3)', 'delta_document.freq_cardiaca_t3-t4', 'document.freq_cardiaca(t-1)', 'document.glicemia_capilar(t-4)', 'delta_document.glicemia_capilar_t2-t3', 'document.temperatura(t-4)', 'delta_document.freq_respiratoria_t1-t2', 'delta_document.pa_sistolica_t1-t2', 'document.glicemia_capilar(t-3)', 'document.glicemia_capilar(t-2)', 'document.glicemia_capilar(t-1)', 'days_from_entrance', 'delta_document.pa_diastolica_t3-t4', 'document.freq_cardiaca(t-2)', 'delta_document.pa_sistolica_t2-t3', 'document.pa_diastolica(t-2)', 'document.pa_diastolica(t-1)', 'delta_document.glicemia_capilar_t3-t4', 'delta_document.glicemia_capilar_t1-t2', 'delta_document.freq_respiratoria_t3-t4', 'delta_document.pa_diastolica_t1-t2', 'document.pa_diastolica(t-3)', 'document.temperatura(t-3)', 'document.sat_o2(t-2)', 'delta_document.temperatura_t2-t3', 'document.sat_o2(t-1)', 'document.freq_cardiaca(t-4)', 'document.temperatura(t-2)']\n",
      "[10:32:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:32:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:32:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:32:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:32:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:32:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:32:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:32:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:32:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:32:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tXGBoost\t 0.888 (+-0.0079) 2.99 s\n",
      "\tLogReg\t 0.8543 (+-0.0111) 3.64 s\n",
      "\tD.Tree\t 0.6789 (+-0.0147) 3.58 s\n",
      "\tRForest\t 0.8616 (+-0.0116) 14.89 s\n",
      "\tCatBoos\t 0.888 (+-0.007) 13.56 s\n",
      "\tNaive\t 0.7596 (+-0.0197) 0.1 s\n",
      "[LightGBM] [Info] Number of positive: 1504, number of negative: 10782\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001526 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7200\n",
      "[LightGBM] [Info] Number of data points in the train set: 12286, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122416 -> initscore=-1.969750\n",
      "[LightGBM] [Info] Start training from score -1.969750\n",
      "[LightGBM] [Info] Number of positive: 1512, number of negative: 10774\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001083 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7192\n",
      "[LightGBM] [Info] Number of data points in the train set: 12286, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.123067 -> initscore=-1.963703\n",
      "[LightGBM] [Info] Start training from score -1.963703\n",
      "[LightGBM] [Info] Number of positive: 1517, number of negative: 10770\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001072 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7206\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.123464 -> initscore=-1.960030\n",
      "[LightGBM] [Info] Start training from score -1.960030\n",
      "[LightGBM] [Info] Number of positive: 1496, number of negative: 10791\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001126 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7227\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.121755 -> initscore=-1.975918\n",
      "[LightGBM] [Info] Start training from score -1.975918\n",
      "[LightGBM] [Info] Number of positive: 1532, number of negative: 10755\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001059 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7196\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124685 -> initscore=-1.948797\n",
      "[LightGBM] [Info] Start training from score -1.948797\n",
      "[LightGBM] [Info] Number of positive: 1493, number of negative: 10794\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001056 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7209\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.121511 -> initscore=-1.978203\n",
      "[LightGBM] [Info] Start training from score -1.978203\n",
      "[LightGBM] [Info] Number of positive: 1524, number of negative: 10763\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001085 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7206\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124034 -> initscore=-1.954776\n",
      "[LightGBM] [Info] Start training from score -1.954776\n",
      "[LightGBM] [Info] Number of positive: 1524, number of negative: 10763\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001097 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7217\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124034 -> initscore=-1.954776\n",
      "[LightGBM] [Info] Start training from score -1.954776\n",
      "[LightGBM] [Info] Number of positive: 1504, number of negative: 10783\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001070 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7209\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122406 -> initscore=-1.969843\n",
      "[LightGBM] [Info] Start training from score -1.969843\n",
      "[LightGBM] [Info] Number of positive: 1532, number of negative: 10755\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001087 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7213\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124685 -> initscore=-1.948797\n",
      "[LightGBM] [Info] Start training from score -1.948797\n",
      "\tLight\t 0.886 (+-0.0086) 1.7 s\n",
      "Number of Columns: 67 Exam(s): 5\n",
      "['document.freq_respiratoria(t-3)', 'document.glicemia_capilar(t)', 'delta_document.pa_sistolica_t3-t4', 'document.freq_cardiaca(t)', 'document.pa_sistolica(t-4)', 'delta_document.sat_o2_t2-t3', 'document.freq_respiratoria(t)', 'delta_document.temperatura_t1-t2', 'delta_document.pa_diastolica_t2-t3', 'age', 'document.sat_o2(t-4)', 'document.pa_sistolica(t-1)', 'document.sat_o2(t)', 'delta_document.sat_o2_t1-t2', 'delta_document.freq_respiratoria_t2-t3', 'delta_document.temperatura_t3-t4', 'document.pa_diastolica(t-4)', 'delta_document.freq_cardiaca_t2-t3', 'document.freq_respiratoria(t-2)', 'document.temperatura(t-1)', 'document.pa_sistolica(t-3)', 'document.freq_respiratoria(t-4)', 'delta_document.temperatura_t-t1', 'document.freq_cardiaca(t-3)', 'delta_document.sat_o2_t-t1', 'document.pa_sistolica(t)', 'delta_document.freq_cardiaca_t1-t2', 'delta_document.sat_o2_t3-t4', 'document.freq_respiratoria(t-1)', 'document.sexo', 'document.pa_diastolica(t)', 'UTI', 'document.pa_sistolica(t-2)', 'document.sat_o2(t-3)', 'delta_document.freq_cardiaca_t3-t4', 'document.freq_cardiaca(t-1)', 'delta_document.pa_sistolica_t-t1', 'document.glicemia_capilar(t-4)', 'delta_document.glicemia_capilar_t2-t3', 'delta_document.freq_cardiaca_t-t1', 'document.temperatura(t-4)', 'delta_document.freq_respiratoria_t1-t2', 'delta_document.pa_sistolica_t1-t2', 'document.glicemia_capilar(t-3)', 'document.glicemia_capilar(t-2)', 'document.glicemia_capilar(t-1)', 'days_from_entrance', 'delta_document.pa_diastolica_t3-t4', 'document.freq_cardiaca(t-2)', 'delta_document.pa_sistolica_t2-t3', 'document.pa_diastolica(t-2)', 'document.pa_diastolica(t-1)', 'delta_document.glicemia_capilar_t3-t4', 'document.temperatura(t)', 'delta_document.glicemia_capilar_t1-t2', 'delta_document.freq_respiratoria_t3-t4', 'delta_document.pa_diastolica_t1-t2', 'delta_document.pa_diastolica_t-t1', 'document.pa_diastolica(t-3)', 'delta_document.glicemia_capilar_t-t1', 'document.temperatura(t-3)', 'document.sat_o2(t-2)', 'delta_document.temperatura_t2-t3', 'document.sat_o2(t-1)', 'delta_document.freq_respiratoria_t-t1', 'document.freq_cardiaca(t-4)', 'document.temperatura(t-2)']\n",
      "[10:33:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:33:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:33:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:33:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:33:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:33:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:33:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:33:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:33:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:33:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tXGBoost\t 0.9065 (+-0.0093) 3.63 s\n",
      "\tLogReg\t 0.8749 (+-0.0086) 4.16 s\n",
      "\tD.Tree\t 0.6954 (+-0.0318) 5.18 s\n",
      "\tRForest\t 0.8857 (+-0.011) 18.3 s\n",
      "\tCatBoos\t 0.9082 (+-0.0068) 13.63 s\n",
      "\tNaive\t 0.7735 (+-0.018) 0.11 s\n",
      "[LightGBM] [Info] Number of positive: 1504, number of negative: 10782\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001871 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9058\n",
      "[LightGBM] [Info] Number of data points in the train set: 12286, number of used features: 67\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122416 -> initscore=-1.969750\n",
      "[LightGBM] [Info] Start training from score -1.969750\n",
      "[LightGBM] [Info] Number of positive: 1512, number of negative: 10774\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001440 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9028\n",
      "[LightGBM] [Info] Number of data points in the train set: 12286, number of used features: 67\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.123067 -> initscore=-1.963703\n",
      "[LightGBM] [Info] Start training from score -1.963703\n",
      "[LightGBM] [Info] Number of positive: 1517, number of negative: 10770\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000742 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9042\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 67\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.123464 -> initscore=-1.960030\n",
      "[LightGBM] [Info] Start training from score -1.960030\n",
      "[LightGBM] [Info] Number of positive: 1496, number of negative: 10791\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9062\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 67\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.121755 -> initscore=-1.975918\n",
      "[LightGBM] [Info] Start training from score -1.975918\n",
      "[LightGBM] [Info] Number of positive: 1532, number of negative: 10755\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001421 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9054\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 67\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124685 -> initscore=-1.948797\n",
      "[LightGBM] [Info] Start training from score -1.948797\n",
      "[LightGBM] [Info] Number of positive: 1493, number of negative: 10794\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001384 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9038\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 67\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.121511 -> initscore=-1.978203\n",
      "[LightGBM] [Info] Start training from score -1.978203\n",
      "[LightGBM] [Info] Number of positive: 1524, number of negative: 10763\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9041\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 67\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124034 -> initscore=-1.954776\n",
      "[LightGBM] [Info] Start training from score -1.954776\n",
      "[LightGBM] [Info] Number of positive: 1524, number of negative: 10763\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9040\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 67\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124034 -> initscore=-1.954776\n",
      "[LightGBM] [Info] Start training from score -1.954776\n",
      "[LightGBM] [Info] Number of positive: 1504, number of negative: 10783\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001417 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9038\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 67\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122406 -> initscore=-1.969843\n",
      "[LightGBM] [Info] Start training from score -1.969843\n",
      "[LightGBM] [Info] Number of positive: 1532, number of negative: 10755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000734 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9077\n",
      "[LightGBM] [Info] Number of data points in the train set: 12287, number of used features: 67\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124685 -> initscore=-1.948797\n",
      "[LightGBM] [Info] Start training from score -1.948797\n",
      "\tLight\t 0.9052 (+-0.0097) 2.04 s\n"
     ]
    }
   ],
   "source": [
    "cols = ['age', 'document.sexo', 'UTI', 'days_from_entrance']\n",
    "t_cols = [c for c in dataset.columns if '4)' in c and (not 'time' in c)]\n",
    "\n",
    "for i in [4,3,2,1,0]:\n",
    "  \n",
    "  if i == 4: cols.extend(t_cols)\n",
    "  if i == 0:\n",
    "    tN_cols = [c for c in dataset.columns if ('t)' in c or '_t-' in c) and (not 'time' in c)]\n",
    "    cols.extend(tN_cols)\n",
    "  else: \n",
    "    tN_cols = [c for c in dataset.columns if ('t-'+str(i) in c or '_t'+str(i) in c) and (not 'time' in c)]\n",
    "    cols.extend(tN_cols)\n",
    "\n",
    "  cols = list(set(cols))\n",
    "  print('Number of Columns:', len(cols), 'Exam(s):', 5-i)\n",
    "  print(cols)\n",
    "\n",
    "  X_W = dataset[cols]\n",
    "  Y_W = dataset[\"outcome\"]\n",
    "\n",
    "  for c in classifiers:\n",
    "    start = time.time()\n",
    "    model = classifiers[c]\n",
    "    scores = cross_val_score(model, X_W, Y_W, cv=kfold, scoring='roc_auc')\n",
    "    print ('\\t' + c + '\\t', round(scores.mean(),4), '(+-' + str(round(scores.std(),4)) + ')', round(time.time() - start,2), 's')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "colab_experiments.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
